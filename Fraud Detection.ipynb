{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unmasking the Web of Deceit: An Analysis of Online Payment Fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Lorem ipsum dolor sit amet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-13T23:54:48.020620Z",
     "iopub.status.busy": "2023-11-13T23:54:48.020210Z",
     "iopub.status.idle": "2023-11-13T23:54:48.032830Z",
     "shell.execute_reply": "2023-11-13T23:54:48.031109Z",
     "shell.execute_reply.started": "2023-11-13T23:54:48.020587Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import os.path\n",
    "import zipfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset\n",
    "This downloads the dataset from Kaggle and reads it into a Pandas DataFrame. See README for instructions on how to set up a Kaggle API key.  Otherwise, download the file and put it in the `/data` folder with the name `online-payments-fraud-detection-dataset.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T23:54:48.036649Z",
     "iopub.status.busy": "2023-11-13T23:54:48.035703Z",
     "iopub.status.idle": "2023-11-13T23:55:09.346609Z",
     "shell.execute_reply": "2023-11-13T23:55:09.345526Z",
     "shell.execute_reply.started": "2023-11-13T23:54:48.036608Z"
    }
   },
   "outputs": [],
   "source": [
    "# Authenticate with your Kaggle credentials\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Specify the dataset name\n",
    "dataset_name = 'online-payments-fraud-detection-dataset'\n",
    "dataset_folder_path = './data/'\n",
    "kaggle_dataset_path = 'rupakroy/' + dataset_name\n",
    "\n",
    "if os.path.isfile(dataset_folder_path + dataset_name + '.csv'):\n",
    "    print(\"Found dataset archive.\")\n",
    "else:\n",
    "    # Download the dataset files\n",
    "    print(\"Downloading dataset from Kaggle.\")\n",
    "    api.dataset_download_files(kaggle_dataset_path, path=dataset_folder_path)\n",
    "\n",
    "    zip_file_path = dataset_folder_path + dataset_name + '.zip'\n",
    "\n",
    "    # Open the zip file\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        # Extract all contents to the specified directory\n",
    "        zip_ref.extractall(dataset_folder_path)\n",
    "\n",
    "    # Delete the zip file\n",
    "    os.remove(zip_file_path)\n",
    "    for filename in zip_ref.namelist():\n",
    "        new_filename = dataset_name + '.csv'\n",
    "        old_filepath = os.path.join(dataset_folder_path, filename)\n",
    "        new_filepath = os.path.join(dataset_folder_path, new_filename)\n",
    "        os.rename(old_filepath, new_filepath)\n",
    "    print(\"Downloaded dataset from Kaggle.\")\n",
    "\n",
    "df = pd.read_csv(dataset_folder_path + dataset_name + '.csv')\n",
    "# make a copy to preserve the original data\n",
    "dff = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for empty data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T23:55:09.348481Z",
     "iopub.status.busy": "2023-11-13T23:55:09.348124Z",
     "iopub.status.idle": "2023-11-13T23:55:11.410804Z",
     "shell.execute_reply": "2023-11-13T23:55:11.409428Z",
     "shell.execute_reply.started": "2023-11-13T23:55:09.348450Z"
    }
   },
   "outputs": [],
   "source": [
    "if dff.isnull().values.any():\n",
    "    print(\"Error: Missing data\")\n",
    "else:\n",
    "    print(\"No missing values found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rename the columns to be easier to understand, based on descriptions of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T23:55:12.033966Z",
     "iopub.status.busy": "2023-11-13T23:55:12.033133Z",
     "iopub.status.idle": "2023-11-13T23:55:13.707304Z",
     "shell.execute_reply": "2023-11-13T23:55:13.705224Z",
     "shell.execute_reply.started": "2023-11-13T23:55:12.033909Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Before: {dff.columns}')\n",
    "\n",
    "dff.columns = ['Transaction_Hours','Type','Transaction_Amount','Sender','Sender_Balance_Previous_Transaction','Sender_Balance_After_Transaction','Receiver','Receiver_Balance_Previous_Transaction','Receiver_Balance_After_Transaction','Is_Fraud', 'Is_Flagged_Fraud']\n",
    "\n",
    "print(f'After: {dff.columns}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T23:55:11.428352Z",
     "iopub.status.busy": "2023-11-13T23:55:11.427982Z",
     "iopub.status.idle": "2023-11-13T23:55:12.031319Z",
     "shell.execute_reply": "2023-11-13T23:55:12.029793Z",
     "shell.execute_reply.started": "2023-11-13T23:55:11.428322Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Before: {dff.columns}')\n",
    "\n",
    "dff = dff.drop('Sender', axis=1)\n",
    "dff = dff.drop('Receiver', axis=1)\n",
    "dff = dff.drop('Is_Flagged_Fraud', axis=1)\n",
    "\n",
    "print(f'After: {dff.columns}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for correct data types\n",
    "We look at type for each column. All columns look good. We explore the Type column to find nominal data which we will convert later. We explore Is_Fraud and find it to correctly be values 0, meaning not fraud, and 1, meaning fraud. We will convert this to a boolean later for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T23:55:11.412826Z",
     "iopub.status.busy": "2023-11-13T23:55:11.412477Z",
     "iopub.status.idle": "2023-11-13T23:55:11.425043Z",
     "shell.execute_reply": "2023-11-13T23:55:11.423823Z",
     "shell.execute_reply.started": "2023-11-13T23:55:11.412796Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print datatypes of features to ensure they are the correct type\n",
    "dff.info()\n",
    "print(dff['Type'].value_counts())\n",
    "print(dff['Is_Fraud'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Nominal Features to Boolean\n",
    "We convert the Type category to columns using one-hot encoding. This creates a new binary column for each category and is appropriate for nominal variables without an intrinsic order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "categorical_columns = dff.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# One-hot encoding: This creates a new binary column for each category and is appropriate for nominal variables without an intrinsic order.\n",
    "dff = pd.get_dummies(dff, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "dff.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Is_Fraud to Boolean type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'dff' is your DataFrame and 'isFraud' is a column with int64 type containing 0s and 1s\n",
    "dff['Is_Fraud'] = dff['Is_Fraud'].astype(bool)\n",
    "# Check the updated data type\n",
    "dff.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for skewed distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-13T23:55:13.711789Z",
     "iopub.status.idle": "2023-11-13T23:55:13.713061Z",
     "shell.execute_reply": "2023-11-13T23:55:13.712750Z",
     "shell.execute_reply.started": "2023-11-13T23:55:13.712717Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "\n",
    "numerical_features = dff.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Create separate histograms with density plots for each numerical column\n",
    "for i, feature in enumerate(numerical_features, 1):\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(3, 3, i)\n",
    "    plt.hist(dff[feature], bins=50, color='skyblue', edgecolor='black')\n",
    "    plt.title(f'Distribution for {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.ticklabel_format(style='plain', axis='x')\n",
    "    plt.ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "\n",
    "# Create separate histograms with density plots for each numerical column\n",
    "for i, feature in enumerate(numerical_features, 1):\n",
    "    # Calculate the IQR\n",
    "    Q1 = dff[feature].quantile(0.25)\n",
    "    Q3 = dff[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define the lower and upper bounds for outliers\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Identify and filter out outliers\n",
    "    df_no_outliers = dff[(dff[feature] >= lower_bound) & (dff[feature] <= upper_bound)]\n",
    "\n",
    "    # Plot histogram of Transaction Amount after removing outliers\n",
    "    plt.subplot(3, 3, i)\n",
    "    plt.hist(df_no_outliers[feature], bins=50, color='skyblue', edgecolor='black')\n",
    "    plt.title(f'{feature} Distribution (No Outliers)')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.ticklabel_format(style='plain', axis='y')\n",
    "    plt.ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = dff['Is_Fraud']\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Plot the class imbalance\n",
    "sns.countplot(x=target, hue=target, palette=[\"skyblue\", \"coral\"], legend=False, edgecolor='black')\n",
    "\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Imbalance in Is_Fraud Column')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform undersampling on the majority class\n",
    "features = dff.drop('Is_Fraud', axis=1)\n",
    "target = dff['Is_Fraud']\n",
    "\n",
    "undersampler = RandomUnderSampler(sampling_strategy='majority', random_state=42)\n",
    "features_undersampled, target_undersampled = undersampler.fit_resample(features, target)\n",
    "\n",
    "# Plot the distribution before and after undersampling\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot the distribution before undersampling\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x=target, hue=target, palette=[\"skyblue\", \"coral\"], legend=False, edgecolor='black')\n",
    "plt.title('Distribution before Undersampling')\n",
    "\n",
    "# Plot the distribution after undersampling\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x=target_undersampled, hue=target_undersampled, palette=[\"skyblue\", \"coral\"], legend=False, edgecolor='black')\n",
    "plt.title('Distribution after Undersampling')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired samples for each class to 100,000\n",
    "desired_samples = 100000\n",
    "\n",
    "sampling_strategy = {0: desired_samples, 1: desired_samples}\n",
    "smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "features_oversampled_smote, target_oversampled_smote = smote.fit_resample(features_undersampled, target_undersampled)\n",
    "\n",
    "# Plot the distribution before and after SMOTE oversampling\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x=target_undersampled, hue=target_undersampled, palette=[\"skyblue\", \"coral\"], legend=False, edgecolor='black')\n",
    "\n",
    "plt.title('Distribution before SMOTE Oversampling')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x=target_oversampled_smote,  hue=target_oversampled_smote, palette=[\"skyblue\", \"coral\"], legend=False, edgecolor='black')\n",
    "plt.title('Distribution after SMOTE Oversampling')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data\n",
    "Use stratify to keep even distribution between sets of class data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the resampled data into training, validation, and two testing sets (80/10/10 split)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(features_oversampled_smote, target_oversampled_smote, test_size=0.2, random_state=42, stratify=target_oversampled_smote)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Plot the class distribution of all three sets\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Training Set Class Distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "pd.Series(y_train).value_counts().plot(kind='bar', color=['skyblue', 'coral'], edgecolor='black')\n",
    "plt.title('Training Set Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Testing Set Class Distribution\n",
    "plt.subplot(1, 3, 2)\n",
    "pd.Series(y_test).value_counts().plot(kind='bar', color=['skyblue', 'coral'], edgecolor='black')\n",
    "plt.title('Testing Set Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Validation Set Class Distribution\n",
    "plt.subplot(1, 3, 3)\n",
    "pd.Series(y_validation).value_counts().plot(kind='bar', color=['skyblue', 'coral'], edgecolor='black')\n",
    "plt.title('Validation set Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = GaussianNB()\n",
    "\n",
    "# Train the Naive Bayes model\n",
    "start_time = time.time()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set for Naive Bayes\n",
    "y_val_pred_nb = nb_model.predict(X_val)\n",
    "\n",
    "# Evaluate the Naive Bayes model on the validation set\n",
    "accuracy_val_nb = accuracy_score(y_val, y_val_pred_nb)\n",
    "conf_matrix_val_nb = confusion_matrix(y_val, y_val_pred_nb)\n",
    "class_report_val_nb = classification_report(y_val, y_val_pred_nb)\n",
    "\n",
    "# Record the run time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "minutes, seconds = divmod(elapsed_time, 60)\n",
    "\n",
    "print(\"Naive Bayes - Validation Set Results:\")\n",
    "print(f\"Accuracy: {accuracy_val_nb:.2f}\")\n",
    "print(f\"{int(minutes)} minutes and {seconds:.2f} seconds\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_val_nb)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report_val_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifiers\n",
    "dt_model = DecisionTreeClassifier()\n",
    "\n",
    "# Train the Decision Trees model\n",
    "start_time = time.time()\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set for Decision Trees\n",
    "y_val_pred_dt = dt_model.predict(X_val)\n",
    "\n",
    "# Evaluate the Decision Trees model on the validation set\n",
    "accuracy_val_dt = accuracy_score(y_val, y_val_pred_dt)\n",
    "conf_matrix_val_dt = confusion_matrix(y_val, y_val_pred_dt)\n",
    "class_report_val_dt = classification_report(y_val, y_val_pred_dt)\n",
    "\n",
    "# Record the run time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "minutes, seconds = divmod(elapsed_time, 60)\n",
    "\n",
    "print(\"\\nDecision Trees - Validation Set Results:\")\n",
    "print(f\"Accuracy: {accuracy_val_dt:.2f}\")\n",
    "print(f\"{int(minutes)} minutes and {seconds:.2f} seconds\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_val_dt)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report_val_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Support Vector Machine Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC()\n",
    "\n",
    "# Train the SVM model\n",
    "start_time = time.time()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set for SVM\n",
    "y_val_pred_svm = svm_model.predict(X_val)\n",
    "\n",
    "# Evaluate the SVM model on the validation set\n",
    "accuracy_val_svm = accuracy_score(y_val, y_val_pred_svm)\n",
    "conf_matrix_val_svm = confusion_matrix(y_val, y_val_pred_svm)\n",
    "class_report_val_svm = classification_report(y_val, y_val_pred_svm)\n",
    "\n",
    "# Record the run time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "minutes, seconds = divmod(elapsed_time, 60)\n",
    "\n",
    "print(\"\\nSVM - Validation Set Results:\")\n",
    "print(f\"Accuracy: {accuracy_val_svm:.2f}\")\n",
    "print(f\"{int(minutes)} minutes and {seconds:.2f} seconds\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_val_svm)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report_val_svm)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2093649,
     "sourceId": 3478314,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30579,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
