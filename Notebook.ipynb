{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unmasking the Web of Deceit: An Analysis of Online Payment Fraud\n",
    "## Data Mining 412/512 Team 4 Project Report\n",
    "\n",
    "### Instructor\n",
    "* Dr. Maninder Singh\n",
    "\n",
    "### Authors\n",
    "* Shawn Eidem - shawn.eidem@go.stcloudstate.edu\n",
    "* William Ortman - william.ortman@go.stcloudstate.edu\n",
    "* Noah Blon - noah.blon@go.stcloudstate.edu\n",
    "* Ashhad Waquad Syed - ashhadwaquas.syed@go.stcloudstate.edu\n",
    "\n",
    "## Overview\n",
    "### Background\n",
    "E-commerce has exploded globally, fueled by a pandemic and the widespread use of mobile phones. In 2023, online transactions are expected to account for nearly 21% of all financial activities in 2023, totaling over 8 trillion dollars [1]. Unfortunately, with this growth has come a huge increase in fraud. In 2022, global losses due to fraud reached 41 billion dollars, and will jump over 48 billion dollars in 2023. By 2027, losses are projected to exceed 343 billion [2].\n",
    "\n",
    "Because the problem is widespread, damaging, and on-the-rise, it is vital to prevent businesses and individuals from becoming victims of fraud. But with such a huge number of transactions, it’s impossible to find fraudulent ones manually. Data mining techniques such as machine learning, are able to sift through huge datasets and find actionable intelligence, making them critical tools to combat the problem.\n",
    "\n",
    "### Research Questions\n",
    "**What are the most accurate machine learning algorithms for binary classification?**\n",
    "\n",
    "We’ll test Naive Bayes, SVM, and Decision Trees algorithms and evaluate them using common metrics to find which score best.\n",
    "\n",
    "**Which algorithms are the most resilient to class imbalance?**\n",
    "\n",
    "We will test Naive Bayes, SVM, and Decision Trees with the original dataset and a sampled dataset to see if there are any differences in performance.\n",
    "\n",
    "### Algorithm Description\n",
    "\n",
    "#### Naive Bayes\n",
    "Naive Bayes is a simple yet effective classification algorithm based on Bayes' theorem. It assumes that features are conditionally independent, given the class label. It is simple and fast, but assuming conditional independence is a downside. [3]\n",
    "\n",
    "#### SVM\n",
    "SVM is a supervised learning algorithm that can be used for classification problems. The primary objective is to find a hyperplane that best separates data points of different classes in the feature space. SVM aims to maximize the margin between classes, making it robust to outliers. [4]\n",
    "\n",
    "### Decision Trees\n",
    "Decision Trees is a supervised machine learning algorithm that can be used for classification problems. It has a hierarchical tree structure consisting of a root node, branches, internal nodes, and leaf nodes. Each node represents the predicted class. [5]\n",
    "\n",
    "\n",
    "### Dataset Description\n",
    "The dataset we will explore is the “Online Payments Fraud Detection” Dataset located on Kaggle [6]. Some features which contributed to our selection of the dataset were:\n",
    "* The dataset features mostly numeric attributes, negating a need for transformations.\n",
    "* We believe there are few or zero missing feature values.\n",
    "* It's substantial, containing over 6 million rows of data. This makes it similar to a real world problem.\n",
    "* The dataset presents a binary classification problem, which simplifies our analysis.\n",
    "\n",
    "Due to privacy reasons, there is a lack of publicly available datasets on mobile money transactions. Therefore, this data was generated through a tool called PaySim, and its features were found comparable to real-world financial data. We are using a subset of the original dataset which included 23 million rows [7].\n",
    "\n",
    "The description of the columns are as follows:\n",
    "\n",
    "* step: represents a unit of time, where 1 step equals 1 hour\n",
    "* type: type of online transaction\n",
    "* amount: the amount of the transaction\n",
    "* nameOrig: customer starting the transaction\n",
    "* oldbalanceOrg: balance before the transaction\n",
    "* newbalanceOrig: balance after the transaction\n",
    "* nameDest: recipient of the transaction\n",
    "* oldbalanceDest: initial balance of recipient before the transaction\n",
    "* newbalanceDest: the new balance of recipient after the transaction\n",
    "* isFraud: fraud transaction flag\n",
    "* isFlaggedFraud: fraud transaction flag for transactions over $200,000\n",
    "\n",
    "The use of a simulated dataset is due to government restrictions preventing access to real-world bank transaction information. PaySim’s synthetic dataset is based on real-world transaction information and is tailored towards the use of machine learning.[6] This dataset is uniquely suited for training as the data is nearly fully complete and considered by many to be a clean data set, with the addition of a column dedicated for isFraud for pre-classified data for easy training purposes. This data is considered highly accurate to real world occurrences and as such is a great match for our project's goals.\n",
    "\n",
    "### Hardware Overview\n",
    "\n",
    "* RAM (Random Access Memory): 32GB\n",
    "* Processor: Apple M1 Pro\n",
    "* Operating System: Mac OS Sonoma\n",
    "* Storage: 500GBs\n",
    "\n",
    "This notebook has been run successfully on this hardware and takes a reasonable time to complete.\n",
    "\n",
    "### Evaluation Plan\n",
    "As a general part of evaluating the project, we compare our trained models and their predictions against the actual data for our selected algorithms. This providse a measure of accuracy and allow us to understand how the models perform. The training, testing, and validation will be done using an 80%/10%/10% split of the data respectively. The metrics collected from testing/training will be the F1 score, precision, and accuracy. We also run our training against a preprocessed, sampled dataset and the original dataset to understand how preprocessing, especially class balancing, effects the performance of our selected classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-13T23:54:48.020620Z",
     "iopub.status.busy": "2023-11-13T23:54:48.020210Z",
     "iopub.status.idle": "2023-11-13T23:54:48.032830Z",
     "shell.execute_reply": "2023-11-13T23:54:48.031109Z",
     "shell.execute_reply.started": "2023-11-13T23:54:48.020587Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import os.path\n",
    "import zipfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, confusion_matrix, classification_report\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset\n",
    "This downloads the dataset from Kaggle and reads it into a Pandas DataFrame. See README for instructions on how to set up a Kaggle API key.  Otherwise, download the file and put it in the `/data` folder with the name `online-payments-fraud-detection-dataset.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T23:54:48.036649Z",
     "iopub.status.busy": "2023-11-13T23:54:48.035703Z",
     "iopub.status.idle": "2023-11-13T23:55:09.346609Z",
     "shell.execute_reply": "2023-11-13T23:55:09.345526Z",
     "shell.execute_reply.started": "2023-11-13T23:54:48.036608Z"
    }
   },
   "outputs": [],
   "source": [
    "# Authenticate with your Kaggle credentials\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Specify the dataset name\n",
    "dataset_name = 'online-payments-fraud-detection-dataset'\n",
    "dataset_folder_path = './data/'\n",
    "kaggle_dataset_path = 'rupakroy/' + dataset_name\n",
    "\n",
    "if os.path.isfile(dataset_folder_path + dataset_name + '.csv'):\n",
    "    print(\"Found dataset archive.\")\n",
    "else:\n",
    "    # Download the dataset files\n",
    "    print(\"Downloading dataset from Kaggle.\")\n",
    "    api.dataset_download_files(kaggle_dataset_path, path=dataset_folder_path)\n",
    "\n",
    "    zip_file_path = dataset_folder_path + dataset_name + '.zip'\n",
    "\n",
    "    # Open the zip file\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        # Extract all contents to the specified directory\n",
    "        zip_ref.extractall(dataset_folder_path)\n",
    "\n",
    "    # Delete the zip file\n",
    "    os.remove(zip_file_path)\n",
    "    for filename in zip_ref.namelist():\n",
    "        new_filename = dataset_name + '.csv'\n",
    "        old_filepath = os.path.join(dataset_folder_path, filename)\n",
    "        new_filepath = os.path.join(dataset_folder_path, new_filename)\n",
    "        os.rename(old_filepath, new_filepath)\n",
    "    print(\"Downloaded dataset from Kaggle.\")\n",
    "\n",
    "df = pd.read_csv(dataset_folder_path + dataset_name + '.csv')\n",
    "# make a copy to preserve the original data\n",
    "dff = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up our analysis for success, we need to analyze, reduce, and manipulate our dataset to achieve the best classification results. First, we’ll inspect and sanitize the data to make sure values are not missing and have the correct type. Our dataset is generated and we expect it to be very clean, but we’ll take this step to mimic actions we’d take on a real-world dataset. Next, we’ll test the data for attributes such as skewness and outliers, and reduce the data where necessary.  Finally, we’ll test for whether our class feature, isFraud, is imbalanced.  If so, we’ll look at techniques such as sampling to ensure we have balanced testing and training data. We may undertake analysis of our algorithms using sampled and unsampled data to test their resistance to imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for empty data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T23:55:09.348481Z",
     "iopub.status.busy": "2023-11-13T23:55:09.348124Z",
     "iopub.status.idle": "2023-11-13T23:55:11.410804Z",
     "shell.execute_reply": "2023-11-13T23:55:11.409428Z",
     "shell.execute_reply.started": "2023-11-13T23:55:09.348450Z"
    }
   },
   "outputs": [],
   "source": [
    "if dff.isnull().values.any():\n",
    "    print(\"Error: Missing data\")\n",
    "else:\n",
    "    print(\"No missing values found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rename the columns to be easier to understand, based on descriptions of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T23:55:12.033966Z",
     "iopub.status.busy": "2023-11-13T23:55:12.033133Z",
     "iopub.status.idle": "2023-11-13T23:55:13.707304Z",
     "shell.execute_reply": "2023-11-13T23:55:13.705224Z",
     "shell.execute_reply.started": "2023-11-13T23:55:12.033909Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Before: {dff.columns}')\n",
    "\n",
    "dff.columns = ['Transaction_Hours','Type','Transaction_Amount','Sender','Sender_Balance_Before_Transaction','Sender_Balance_After_Transaction','Receiver','Receiver_Balance_Before_Transaction','Receiver_Balance_After_Transaction','Is_Fraud', 'Is_Flagged_Fraud']\n",
    "\n",
    "print(f'After: {dff.columns}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop irrelevant columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop columns that aren't needed for analysis:\n",
    "* Sender - the ID of the initiator of the transaction\n",
    "* Receiver - the ID of the receiver of the transaction\n",
    "* Is_Flagged_Fraud - a classification field containing an extremely low number of positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T23:55:11.428352Z",
     "iopub.status.busy": "2023-11-13T23:55:11.427982Z",
     "iopub.status.idle": "2023-11-13T23:55:12.031319Z",
     "shell.execute_reply": "2023-11-13T23:55:12.029793Z",
     "shell.execute_reply.started": "2023-11-13T23:55:11.428322Z"
    }
   },
   "outputs": [],
   "source": [
    "print(dff['Is_Flagged_Fraud'].value_counts())\n",
    "\n",
    "print(f'Before: {dff.columns}')\n",
    "\n",
    "dff = dff.drop('Sender', axis=1)\n",
    "dff = dff.drop('Receiver', axis=1)\n",
    "dff = dff.drop('Is_Flagged_Fraud', axis=1)\n",
    "\n",
    "print(f'After: {dff.columns}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for correct data types\n",
    "We look at type for each column and find that the data types look appropriate. We also explore the Type column to find nominal data which we will convert later. We explore Is_Fraud and find it to correctly be values 0, meaning not fraudulent, and 1, meaning fraudulent. We will convert this to a boolean later for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T23:55:11.412826Z",
     "iopub.status.busy": "2023-11-13T23:55:11.412477Z",
     "iopub.status.idle": "2023-11-13T23:55:11.425043Z",
     "shell.execute_reply": "2023-11-13T23:55:11.423823Z",
     "shell.execute_reply.started": "2023-11-13T23:55:11.412796Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print datatypes of features to ensure they are the correct type\n",
    "dff.info()\n",
    "print('\\n Type Value Counts: \\n\\n')\n",
    "print(dff['Type'].value_counts())\n",
    "print('\\n Fraud Value Counts: \\n\\n')\n",
    "print(dff['Is_Fraud'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Nominal Features to Boolean\n",
    "We convert the Type category to columns using one-hot encoding. This creates a new binary column for each category and is a better choice for nominal variables where there is no intrinsic order. [8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = dff.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# One-hot encoding: This creates a new binary column for each category and is appropriate for nominal variables without an intrinsic order.\n",
    "dff = pd.get_dummies(dff, columns=categorical_columns)\n",
    "\n",
    "dff.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Is_Fraud to Boolean type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['Is_Fraud'] = dff['Is_Fraud'].astype(bool)\n",
    "# Check the updated data type\n",
    "dff.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for skewed distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explore the numeric data for skewed distributions and Transaction_Amount, Sender_Balance_Before_Transaction, Sender_Balance_After_Transaction, Receiver_Balance_Before_Transaction, Receiver_Balance_After_Transaction are skewed right. We'll employ methods to correct the skewness for better results of our classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-13T23:55:13.711789Z",
     "iopub.status.idle": "2023-11-13T23:55:13.713061Z",
     "shell.execute_reply": "2023-11-13T23:55:13.712750Z",
     "shell.execute_reply.started": "2023-11-13T23:55:13.712717Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "\n",
    "numerical_features = dff.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Create separate histograms with density plots for each numerical column\n",
    "for i, feature in enumerate(numerical_features, 1):\n",
    "    plt.subplot(3, 2, i)\n",
    "    plt.hist(dff[feature], bins=50, color='skyblue', edgecolor='black')\n",
    "    plt.title(f'Distribution for {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.ticklabel_format(style='plain', axis='x')\n",
    "    plt.ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove the outliers, we'll use the IQR range and preserve data within the middle 50% of data points. To calculate the quartiles, we look at the range of the positive fraud data rather than the entire dataset, which should lead to better classification results. Interestingly, using this method, we find that the Sender_Balance_After_Transaction is mostly around 0 for Is_Fraud transactions. This makes sense, as accounts with higher balances are likely not engaging in fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,18))\n",
    "\n",
    "skewed_features = numerical_features.drop('Transaction_Hours')\n",
    "\n",
    "positive_fraud_data = dff[dff['Is_Fraud'] == 1]\n",
    "\n",
    "# Create separate histograms with density plots for each numerical column\n",
    "for i, feature in enumerate(skewed_features, 1):\n",
    "    # Calculate the IQR\n",
    "    Q1 = positive_fraud_data[feature].quantile(0.25)\n",
    "    Q3 = positive_fraud_data[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define the lower and upper bounds for outliers\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Identify and filter out outliers\n",
    "    df_no_outliers = dff[(dff[feature] >= lower_bound) & (dff[feature] <= upper_bound)]\n",
    "\n",
    "    # Plot histogram of Transaction Amount after removing outliers\n",
    "    plt.subplot(3, 2, i)\n",
    "    plt.hist(df_no_outliers[feature], bins=50, color='skyblue', edgecolor='black')\n",
    "    plt.title(f'{feature} Distribution (No Outliers)')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.ticklabel_format(style='plain', axis='y')\n",
    "    plt.ticklabel_format(style='plain', axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now check for class imbalance, where the main class, in this case Is_Fraud, is rare. Checking the count of values, we see that the in fact our class is highly imbalanced and correcting the class imbalance should help our classification models.\n",
    "\n",
    "We can employ sampling, where we change the training data distribution so that the Is_Fraud is well represented. Undersampling decreases the number of negative tuples to align with the positive ones. Oversampling creates new positive tuples to that there are an equal number of positive and negatives. [9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df_no_outliers['Is_Fraud']\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Plot the class imbalance\n",
    "sns.countplot(x=target, hue=target, palette=[\"skyblue\", \"coral\"], legend=False, edgecolor='black')\n",
    "\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Imbalance in Is_Fraud Column')\n",
    "plt.ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersample data\n",
    "First we reduce the number of negative sample to match the number of positive ones by undersampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform undersampling on the majority class\n",
    "features = df_no_outliers.drop('Is_Fraud', axis=1)\n",
    "target = df_no_outliers['Is_Fraud']\n",
    "\n",
    "# The majority class is typically the class with more instances in an imbalanced dataset. \n",
    "# Undersampling aims to balance the class distribution by reducing the number of instances in the majority class.\n",
    "undersampler = RandomUnderSampler(sampling_strategy='majority', random_state=42)\n",
    "features_undersampled, target_undersampled = undersampler.fit_resample(features, target)\n",
    "\n",
    "# Plot the distribution before and after undersampling\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot the distribution before undersampling\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x=target, hue=target, palette=[\"skyblue\", \"coral\"], legend=False, edgecolor='black')\n",
    "plt.title('Distribution before Undersampling')\n",
    "plt.ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "# Plot the distribution after undersampling\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x=target_undersampled, hue=target_undersampled, palette=[\"skyblue\", \"coral\"], legend=False, edgecolor='black')\n",
    "plt.title('Distribution after Undersampling')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample the data\n",
    "Having achieved class balance, we use SMOTE - Synthetic Minority Sampling Technique - to boost the sample count to 100,000, a threshold where our classification models should perform better. [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired samples for each class to 100,000\n",
    "desired_samples = 100000\n",
    "\n",
    "# Keys represent class labels, and values represent the desired number of samples for each class after oversampling.\n",
    "sampling_strategy = {0: desired_samples, 1: desired_samples}\n",
    "smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "features_oversampled_smote, target_oversampled_smote = smote.fit_resample(features_undersampled, target_undersampled)\n",
    "\n",
    "# Plot the distribution before and after SMOTE oversampling\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x=target_undersampled, hue=target_undersampled, palette=[\"skyblue\", \"coral\"], legend=False, edgecolor='black')\n",
    "\n",
    "plt.title('Distribution before SMOTE Oversampling')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x=target_oversampled_smote,  hue=target_oversampled_smote, palette=[\"skyblue\", \"coral\"], legend=False, edgecolor='black')\n",
    "plt.title('Distribution after SMOTE Oversampling')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Now that we have completed classification, we can begin training and testign against our three algorithms, Naive Bayes, SVM, and Decision Trees. \n",
    "### Split Data\n",
    "First we must split the data between our training, testing, and validation sets. We keep an even distribution of our positive and negative class labels between these sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the resampled data into training, validation, and two testing sets (80/10/10 split)\n",
    "# The stratify parameter ensures that the splitting of the dataset maintains the same distribution of target classes as the original dataset.\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(features_oversampled_smote, target_oversampled_smote, test_size=0.2, random_state=42, stratify=target_oversampled_smote)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Plot the class distribution of all three sets\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Training Set Class Distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "# Series converts a one-dimensional array or list into a pandas Series, which is a labeled one-dimensional array.\n",
    "pd.Series(y_train).value_counts().plot(kind='bar', color=['skyblue', 'coral'], edgecolor='black')\n",
    "plt.title('Training Set Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Testing Set Class Distribution\n",
    "plt.subplot(1, 3, 2)\n",
    "pd.Series(y_test).value_counts().plot(kind='bar', color=['skyblue', 'coral'], edgecolor='black')\n",
    "plt.title('Testing Set Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Validation Set Class Distribution\n",
    "plt.subplot(1, 3, 3)\n",
    "pd.Series(y_val).value_counts().plot(kind='bar', color=['skyblue', 'coral'], edgecolor='black')\n",
    "plt.title('Validation set Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run train and test the Naive Bayes algorithm with our sampled data. We generate a confusion matrix and accuracy, precision, and recall metrics in order to evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the Naive Bayes model\n",
    "nb_model = GaussianNB()\n",
    "start_time = time.time()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set for Naive Bayes\n",
    "y_val_pred_nb = nb_model.predict(X_val)\n",
    "\n",
    "# Evaluate the Naive Bayes model on the validation set\n",
    "accuracy_val_nb = accuracy_score(y_val, y_val_pred_nb)\n",
    "conf_matrix_val_nb = confusion_matrix(y_val, y_val_pred_nb)\n",
    "class_report_val_nb = classification_report(y_val, y_val_pred_nb, digits=3)\n",
    "\n",
    "# Record the run time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "minutes, seconds = divmod(elapsed_time, 60)\n",
    "\n",
    "print(\"\\nNaive Bayes - Validation Set Results:\")\n",
    "print(f\"Accuracy: {accuracy_val_nb:.2f}\")\n",
    "print(f\"{int(minutes)} minutes and {seconds:.2f} seconds\")\n",
    "\n",
    "# Print a prettier confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_val_nb, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, annot_kws={\"size\": 14})\n",
    "plt.title(\"Confusion Matrix - Naive Bayes\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(class_report_val_nb)\n",
    "\n",
    "nb_time = elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run train and test the Decision Trees algorithm with our sampled data. We generate a confusion matrix and accuracy, precision, and recall metrics in order to evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the Decision Trees model\n",
    "dt_model = DecisionTreeClassifier()\n",
    "start_time = time.time()\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set for Decision Trees\n",
    "y_val_pred_dt = dt_model.predict(X_val)\n",
    "\n",
    "# Evaluate the Decision Trees model on the validation set\n",
    "accuracy_val_dt = accuracy_score(y_val, y_val_pred_dt)\n",
    "conf_matrix_val_dt = confusion_matrix(y_val, y_val_pred_dt)\n",
    "class_report_val_dt = classification_report(y_val, y_val_pred_dt, digits=3)\n",
    "\n",
    "# Record the run time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "minutes, seconds = divmod(elapsed_time, 60)\n",
    "\n",
    "print(\"\\nDecision Trees - Validation Set Results:\")\n",
    "print(f\"Accuracy: {accuracy_val_dt:.2f}\")\n",
    "print(f\"{int(minutes)} minutes and {seconds:.2f} seconds\")\n",
    "\n",
    "# Print a prettier confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_val_dt, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, annot_kws={\"size\": 14})\n",
    "plt.title(\"Confusion Matrix - Decision Trees\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(class_report_val_dt)\n",
    "\n",
    "dt_time = elapsed_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine Classifier\n",
    "Now we run train and test the Support Vector Machine algorithm with our sampled data. We generate a confusion matrix and accuracy, precision, and recall metrics in order to evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the SVM model\n",
    "svm_model = SVC()\n",
    "start_time = time.time()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set for SVM\n",
    "y_val_pred_svm = svm_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the SVM model on the validation set\n",
    "accuracy_val_svm = accuracy_score(y_val, y_val_pred_svm)\n",
    "conf_matrix_val_svm = confusion_matrix(y_val, y_val_pred_svm)\n",
    "class_report_val_svm = classification_report(y_val, y_val_pred_svm, digits=3)\n",
    "\n",
    "# Record the run time\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "minutes, seconds = divmod(elapsed_time, 60)\n",
    "\n",
    "print(\"\\nSVM - Validation Set Results:\")\n",
    "print(f\"Accuracy: {accuracy_val_svm:.2f}\")\n",
    "print(f\"{int(minutes)} minutes and {seconds:.2f} seconds\")\n",
    "\n",
    "# Print a prettier confusion matrix using seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_val_svm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, annot_kws={\"size\": 14})\n",
    "plt.title(\"Confusion Matrix - SVM\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(class_report_val_svm)\n",
    "\n",
    "svm_time = elapsed_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the models on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the models on the test set\n",
    "y_test_pred_nb = nb_model.predict(X_test)\n",
    "y_test_pred_svm = svm_model.predict(X_test)\n",
    "y_test_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "print(\"Naive Bayes - Test Set Results:\")\n",
    "print(classification_report(y_val, y_test_pred_nb, digits=3));\n",
    "\n",
    "print(\"SVM - Test Set Results:\")\n",
    "print(classification_report(y_val, y_test_pred_svm, digits=3));\n",
    "\n",
    "print(\"Decision Trees - Test Set Results:\")\n",
    "print(classification_report(y_val, y_test_pred_dt, digits=3));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_nb = accuracy_score(y_test, y_test_pred_nb)\n",
    "accuracy_svm = accuracy_score(y_test, y_test_pred_svm)\n",
    "accuracy_dt = accuracy_score(y_test, y_test_pred_dt)\n",
    "\n",
    "f1_score_positive_nb = f1_score(y_test, y_test_pred_nb, pos_label=1)\n",
    "f1_score_negative_nb = f1_score(y_test, y_test_pred_nb, pos_label=0)\n",
    "\n",
    "f1_score_positive_svm = f1_score(y_test, y_test_pred_svm, pos_label=1)\n",
    "f1_score_negative_svm = f1_score(y_test, y_test_pred_svm, pos_label=0)\n",
    "\n",
    "f1_score_positive_dt = f1_score(y_test, y_test_pred_dt, pos_label=1)\n",
    "f1_score_negative_dt = f1_score(y_test, y_test_pred_dt, pos_label=0)\n",
    "\n",
    "precision_positive_nb = precision_score(y_test, y_test_pred_nb, pos_label=1)\n",
    "precision_negative_nb = precision_score(y_test, y_test_pred_nb, pos_label=0)\n",
    "\n",
    "precision_positive_svm = precision_score(y_test, y_test_pred_svm, pos_label=1)\n",
    "precision_negative_svm = precision_score(y_test, y_test_pred_svm, pos_label=0)\n",
    "\n",
    "precision_positive_dt = precision_score(y_test, y_test_pred_dt, pos_label=1)\n",
    "precision_negative_dt = precision_score(y_test, y_test_pred_dt, pos_label=0)\n",
    "\n",
    "# Classifier names\n",
    "classifiers = ['Naive Bayes', 'SVM', 'Decision Trees']\n",
    "# Metrics values\n",
    "accuracy_values = [accuracy_nb, accuracy_svm, accuracy_dt]\n",
    "precision_positive_values = [precision_positive_nb, precision_positive_svm, precision_positive_dt]\n",
    "precision_negative_values = [precision_negative_nb, precision_negative_svm, precision_negative_dt]\n",
    "f1_score_positive_values = [f1_score_positive_nb, f1_score_positive_svm, f1_score_positive_dt]\n",
    "f1_score_negative_values = [f1_score_negative_nb, f1_score_negative_svm, f1_score_negative_dt]\n",
    "\n",
    "# Start plotting all four bar charts in a single figure\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "# Plotting Accuracy\n",
    "plt.bar(classifiers, accuracy_values, color=['skyblue', 'coral', 'springgreen'], edgecolor=['black', 'black', 'black'])\n",
    "plt.title('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "# Plotting Precision for Positive Class\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.bar(classifiers, precision_positive_values, color=['skyblue', 'coral', 'springgreen'], edgecolor=['black', 'black', 'black'])\n",
    "plt.title('Precision (Positive Class)')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Plotting Precision for Negative Class\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.bar(classifiers, precision_negative_values, color=['skyblue', 'coral', 'springgreen'], edgecolor=['black', 'black', 'black'])\n",
    "plt.title('Precision (Negative Class)')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Plotting F1 Score for Positive Class\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.bar(classifiers, f1_score_positive_values, color=['skyblue', 'coral', 'springgreen'], edgecolor=['black', 'black', 'black'])\n",
    "plt.title('F1 Score (Positive Class)')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.bar(classifiers, f1_score_negative_values, color=['skyblue', 'coral', 'springgreen'], edgecolor=['black', 'black', 'black'])\n",
    "plt.title('F1 Score (Negative Class)')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "\n",
    "# Show the figure with subplots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Time Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time taken to train Naive Bayes Model')\n",
    "minutes, seconds = divmod(nb_time, 60)\n",
    "print(f\"{int(minutes)} minutes and {seconds:.2f} seconds\\n\")\n",
    "\n",
    "print('Time taken to train Decision Trees Model')\n",
    "minutes, seconds = divmod(dt_time, 60)\n",
    "print(f\"{int(minutes)} minutes and {seconds:.2f} seconds\\n\")\n",
    "\n",
    "print('Time taken to train SVM Model')\n",
    "minutes, seconds = divmod(svm_time, 60)\n",
    "print(f\"{int(minutes)} minutes and {seconds:.2f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Question 1: Results and Discussion\n",
    "**What are the most accurate machine learning algorithms for binary classification?**\n",
    "\n",
    "The results of this research question are obtained using the Naive Bayes, SVM, and Decision Trees algorithms. The results are evaluated using the metrics described as part of the research question and evaluation plan sections.\n",
    "\n",
    "The results highlight that the decision trees algorithm is the strongest fit of the three algorithms under test. While Naive Bayes was the fastest to train and test, its poor classification metrics would result in many missed fraudulent activities and false positives. Looking at SVM, it was the slowest performing and while it did have better classification results over Naive Bayes, its slow nature grows exponentially worse with data size in comparison to the other two algorithms. However, while SVM performed far better than the Naive Bayes classification-wise, it still fell short of the decision trees algorithm. Decision trees had a marginally worse training and testing time compared to Naive Bayes, but in return boasted the strongest classification results of the three algorithms. This highlights that in regards to our three algorithms and for the application of fraud prevention, decision trees should be a clear choice.\n",
    "\n",
    "The time differences can be attributed to the time complexity derived from the implementation of the algorithms chosen.  Naive Bayes’ time complexity can be calculated as O(n*m*c), where n is the number of samples, m is the number of features, and c is the number of classes [11]. The time complexity of SVM is between O(m*n^2) and O(m*n^3) where n is the number of samples and m is the number of features [12]. The final algorithm, decision trees, has a time complexity of  O(n*m*log(n), where n is the number of samples and m is the number of features  [13]. As can be seen, in table 1, this results in SVM’s performance scaling poorly with datasize, but scales acceptably well with both decision trees and naive bayes. \n",
    "\n",
    "The difference in classification metrics can be attributed to how the algorithms work. Naive Bayes performs poorly as it does not assume any relationships between our data. Instead, it  takes probabilities into consideration as classification. Our belief on the difference between SVM and decision trees is that SVM is more susceptible to outliers, thus dropping its classification metrics compared to the decision trees which seems to be more resilient. SVM’s core algorithm is plotting data using all attributes into an n-dimensional plane, causing every feature to be accounted for when predicting. This results in the boundary being a more impactful part of the classification process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Question 2: Setup\n",
    "To answer Research Question 2, we need to compare the results of our training a classification model on the original dataset against our results on the sampled dataset. We'll start by training the data. For SVM, we are using precomputed data since the algorithm takes a significant amount of time to run. The code for this can be found in the `ProjectAlgo.py` file in the git repository for the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the file\n",
    "data = df.copy()\n",
    "\n",
    "# Separate features and target variable\n",
    "data['type'] = data['type'].map({'CASH_OUT':1, 'PAYMENT':2, 'CASH_IN':3, 'TRANSFER':4, 'DEBIT':5})\n",
    "X = data[['step', 'type', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']]\n",
    "y = data['isFraud']\n",
    "\n",
    "#map non-numeric columns\n",
    "\n",
    "# Split the data into training (80%), validation (10%), and testing (10%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize the classifiers\n",
    "nb_model = GaussianNB()\n",
    "dt_model = DecisionTreeClassifier()\n",
    "\n",
    "# Train the Naive Bayes model\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Train the Decision Trees model\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Test the models on the test set\n",
    "y_test_pred_original_nb = nb_model.predict(X_test)\n",
    "y_test_pred_original_dt = dt_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_original_nb = accuracy_score(y_test, y_test_pred_original_nb)\n",
    "accuracy_original_svm = 1.00\n",
    "accuracy_original_dt = accuracy_score(y_test, y_test_pred_original_dt)\n",
    "\n",
    "f1_score_positive_original_nb = f1_score(y_test, y_test_pred_original_nb, pos_label=1)\n",
    "f1_score_negative_original_nb = f1_score(y_test, y_test_pred_original_nb, pos_label=0)\n",
    "\n",
    "f1_score_positive_original_svm = 0.52\n",
    "f1_score_negative_original_svm = 1.00\n",
    "\n",
    "f1_score_positive_original_dt = f1_score(y_test, y_test_pred_original_dt, pos_label=1)\n",
    "f1_score_negative_original_dt = f1_score(y_test, y_test_pred_original_dt, pos_label=0)\n",
    "\n",
    "precision_positive_original_nb = precision_score(y_test, y_test_pred_original_nb, pos_label=1)\n",
    "precision_negative_original_nb = precision_score(y_test, y_test_pred_original_nb, pos_label=0)\n",
    "\n",
    "precision_positive_original_svm = 0.99\n",
    "precision_negative_original_svm = 1.00\n",
    "\n",
    "precision_positive_original_dt = precision_score(y_test, y_test_pred_original_dt, pos_label=1)\n",
    "precision_negative_original_dt = precision_score(y_test, y_test_pred_original_dt, pos_label=0)\n",
    "\n",
    "# Classifier names\n",
    "classifiers = [\n",
    "    'Naive Bayes Unsampled', \n",
    "    'Naive Bayes Sampled',\n",
    "    'SVM Unsampled',\n",
    "    'SVM Sampled',\n",
    "    'Decision Trees Sampled',\n",
    "    'Decision Trees Unsampled'\n",
    "]\n",
    "\n",
    "# Metrics values\n",
    "accuracy_graph_values = [\n",
    "    accuracy_original_nb,\n",
    "    accuracy_nb,\n",
    "    accuracy_original_svm, \n",
    "    accuracy_svm,\n",
    "    accuracy_original_dt,\n",
    "    accuracy_dt\n",
    "]\n",
    "\n",
    "precision_positive_graph_values = [\n",
    "    precision_positive_original_nb,\n",
    "    precision_positive_nb,\n",
    "    precision_positive_original_svm,\n",
    "    precision_positive_svm,\n",
    "    precision_positive_original_dt,\n",
    "    precision_positive_dt\n",
    "]\n",
    "\n",
    "precision_negative_graph_values = [\n",
    "    precision_negative_original_nb,\n",
    "    precision_negative_nb,\n",
    "    precision_negative_original_svm,\n",
    "    precision_negative_svm,\n",
    "    precision_negative_original_dt,\n",
    "    precision_negative_dt\n",
    "]\n",
    "\n",
    "f1_score_positive_graph_values = [\n",
    "    f1_score_positive_original_nb,\n",
    "    f1_score_positive_nb,\n",
    "    f1_score_positive_original_svm,\n",
    "    f1_score_positive_svm,\n",
    "    f1_score_positive_original_dt,\n",
    "    f1_score_positive_dt\n",
    "]\n",
    "\n",
    "f1_score_negative_graph_values = [\n",
    "    f1_score_negative_original_nb,\n",
    "    f1_score_negative_nb,\n",
    "    f1_score_negative_original_svm,\n",
    "    f1_score_negative_svm,\n",
    "    f1_score_negative_original_dt,\n",
    "    f1_score_negative_dt\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "# Defining the classifier names for the x-axis\n",
    "classifiers_nb = ['Naive Bayes Unsampled', 'Naive Bayes Sampled']\n",
    "classifiers_svm = ['SVM Unsampled', 'SVM Sampled']\n",
    "classifiers_dt = ['Decision Trees Unsampled', 'Decision Trees Sampled']\n",
    "\n",
    "# Creating accuracy values specific to each classifier type\n",
    "accuracy_values_nb = [accuracy_original_nb, accuracy_nb]\n",
    "accuracy_values_svm = [accuracy_original_svm, accuracy_svm]\n",
    "accuracy_values_dt = [accuracy_original_dt, accuracy_dt]\n",
    "\n",
    "# Subplot for Naive Bayes\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(classifiers_nb, accuracy_values_nb, color=['darkblue', 'skyblue'], edgecolor='black')\n",
    "plt.title('Naive Bayes - Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Subplot for SVM\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(classifiers_svm, accuracy_values_svm, color=['red', 'coral'], edgecolor='black')\n",
    "plt.title('SVM - Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Subplot for Decision Trees\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(classifiers_dt, accuracy_values_dt, color=['darkgreen', 'springgreen'], edgecolor='black')\n",
    "plt.title('Decision Trees - Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Adjust layout for better spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot for Precision (Negative Class) for Naive Bayes\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(classifiers_nb, [precision_negative_original_nb, precision_negative_nb], color=['darkblue', 'skyblue'], edgecolor='black')\n",
    "plt.title('Naive Bayes - Precision (Negative Class)')\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Precision')\n",
    "\n",
    "# Subplot for Precision (Negative Class) for SVM\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(classifiers_svm, [precision_negative_original_svm, precision_negative_svm], color=['red', 'coral'], edgecolor='black')\n",
    "plt.title('SVM - Precision (Negative Class)')\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Precision')\n",
    "\n",
    "# Subplot for Precision (Negative Class) for Decision Trees\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(classifiers_dt, [precision_negative_original_dt, precision_negative_dt], color=[ 'darkgreen', 'springgreen'], edgecolor='black')\n",
    "plt.title('Decision Trees - Precision (Negative Class)')\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Precision')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot for F1 Score (Negative Class) for Naive Bayes\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(classifiers_nb, [f1_score_negative_original_nb, f1_score_negative_nb], color=['darkblue', 'skyblue'], edgecolor='black')\n",
    "plt.title('Naive Bayes - F1 Score (Negative Class)')\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('F1 Score')\n",
    "\n",
    "# Subplot for F1 Score (Negative Class) for SVM\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(classifiers_svm, [f1_score_negative_original_svm, f1_score_negative_svm], color=['red', 'coral'], edgecolor='black')\n",
    "plt.title('SVM - F1 Score (Negative Class)')\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('F1 Score')\n",
    "\n",
    "# Subplot for F1 Score (Negative Class) for Decision Trees\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(classifiers_dt, [f1_score_negative_original_dt, f1_score_negative_dt], color=[ 'darkgreen', 'springgreen'], edgecolor='black')\n",
    "plt.title('Decision Trees - F1 Score (Negative Class)')\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('F1 Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot for Precision (Positive Class) for Naive Bayes\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "# Subplot for Precision (Positive Class) for Naive Bayes\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(classifiers_nb, [precision_positive_original_nb, precision_positive_nb], color=['darkblue', 'skyblue'], edgecolor='black')\n",
    "plt.title('Naive Bayes - Precision (Positive Class)')\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Precision')\n",
    "\n",
    "# Subplot for Precision (Positive Class) for SVM\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(classifiers_svm, [precision_positive_original_svm, precision_positive_svm], color=['red', 'coral'], edgecolor='black')\n",
    "plt.title('SVM - Precision (Positive Class)')\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Precision')\n",
    "\n",
    "# Subplot for Precision (Positive Class) for Decision Trees\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(classifiers_dt, [precision_positive_original_dt, precision_positive_dt], color=[ 'darkgreen', 'springgreen'], edgecolor='black')\n",
    "plt.title('Decision Trees - Precision (Positive Class)')\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Precision')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot for F1_score (Positive Class) for Naive Bayes\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(classifiers_nb, [f1_score_positive_original_nb, f1_score_positive_nb], color=['darkblue', 'skyblue'], edgecolor='black')\n",
    "plt.title('Naive Bayes - F1 Score (Positive Class)')\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('F1 Score')\n",
    "\n",
    "# Subplot for F1 Score (Positive Class) for SVM\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(classifiers_svm, [f1_score_positive_original_svm, f1_score_positive_svm], color=['red', 'coral'], edgecolor='black')\n",
    "plt.title('SVM - F1_score (Positive Class)')\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('F1 Score')\n",
    "\n",
    "# Subplot for F1 Score (Positive Class) for Decision Trees\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(classifiers_dt, [f1_score_positive_original_dt, f1_score_positive_dt], color=[ 'darkgreen', 'springgreen'], edgecolor='black')\n",
    "plt.title('Decision Trees - F1 Score (Positive Class)')\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('F1 Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Question 2 - Results & Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results imply that there is a substantial improvement in the performance of each of the three models when the class imbalance was removed. The implementation of removing class imbalance had a massive impact on even the worst-performing algorithm, Naive Bayes, which had an increase from 4% to 93% in precision for the positive label of the isFraud class and improvement from 6% to 63% in F1-score.\n",
    "\n",
    "SVM-trained model’s F1-score went up from 52% to 89% for positive isFraud class.\n",
    "\n",
    "The most resilient to class imbalance of the three models, was the one trained on Decision Trees which had only 9% and 11% increase albeit still an improvement. \n",
    "\n",
    "We have theorized these outcomes based on the way each algorithm works in the background. The decision trees would deduce the outcome hierarchically considering each attribute relevant to their impact, whereas Naive Bayes would have every attribute contributing to the outcome equally with their conditional probabilities so even the attributes that do not significantly impact the result would still interfere with the results. Likewise, the Support Vector Machine classifier would also have each attribute be given equal significance as it draws the boundary separating the two classes based on where the data points are placed in the n-dimensional plane, ‘n’ being the number of attributes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "### Experiment Takeaways\n",
    "This experiment had some key takeaways we can derive from our results. Preprocessing the data had a noticeable impact on algorithm performance as can be seen from the difference in performance for the Naive Bayes. Additionally, we can see that SVM takes a far longer time to run as the dataset increases in size making it less suited for large data sets. In the case of fraud classification - we can see that the decision tree algorithm is a strong performer due to its strong accuracy, precision, F1 score, and run time in comparison to the other algorithms tested. Finally, it can be observed that it was easier to predict the non-fraud case against the fraud cases.\n",
    "\n",
    "### Future Work\n",
    "To further expand on the results of this experiment, multiple aspects of the experiment could be modified. The first method to expand on the results would be to re-run the experiment with an increased number of algorithm types outside of our selected algorithms. Additionally, it would be beneficial to add time scaling to determine where the algorithms take longest instead of measuring only the overall process for the algorithm. Another improvement to be made would be to train and test against the full Paysim data set to have a larger data set to work with. While more improvements could be made, these were key points of improvements and expansion for the experiment as it is currently.\n",
    "\n",
    "### Closing Thoughts\n",
    "This project was an effective learning exercise to step into the world of classification and fraud detection.  The journey has not only deepened our understanding of machine learning techniques but has also underscored the significance of collaborative efforts in combating financial fraud. As we draw the curtains on this endeavor, it is imperative to acknowledge the challenges encountered and the lessons learned. The application of machine learning in the realm of fraud detection represents a dynamic and evolving field, necessitating ongoing research and adaptation to stay ahead of increasingly sophisticated fraudulent tactics. Ultimately, this project serves as a stepping stone, contributing to the broader mission of enhancing the security of financial systems through the innovative integration of technology and data analytics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] A. Baluch, “38 E-Commerce Statistics Of 2023”, Forbes.com 2023. [Online]. Available: https://www.forbes.com/advisor/business/ecommerce-statistics/ [Accessed: October 25, 2023]\n",
    "\n",
    "[2] Mastercard Identity, “Ecommerce fraud trends and statistics merchants need to know in 2023”, Ekata by Mastercard 2023. [Online]. Available: https://ekata.com/blog/ecommerce-fraud-trends-and-statistics-merchants-need-to-know-in-2023/ [Accessed: October 25, 2023]\n",
    "\n",
    "[3] S. Ray \"Naive Bayes Classifier Explained: Applications and Practice Problems of Naive Bayes Classifier\", AnalyticsVidhya. [Online]. Available: https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/  [Accessed: December 1, 2023]\n",
    "\n",
    "[4] A. Saini \"Guide on Support Vector Machine (SVM) Algorithm\", AnalyticsVidhya. [Online]. Available: https://www.analyticsvidhya.com/blog/2021/10/support-vector-machinessvm-a-complete-guide-for-beginners/ [Accessed: December 1, 2023]\n",
    "\n",
    "[5] A. Saini \"Decision Tree Algorithm – A Complete Guide\n",
    "\", AnalyticsVidhya. [Online]. Available: https://www.analyticsvidhya.com/blog/2021/08/decision-tree-algorithm/  [Accessed: December 1, 2023]\n",
    "\n",
    "[6] R. Roy, ”Online Payments Fraud Detection Dataset\" [Online]. Available: https://www.kaggle.com/datasets/rupakroy/online-payments-fraud-detection-dataset. [Accessed: October 25, 2023].\n",
    "\n",
    "[7] E. A. Lopez-Rojas, A. Elmir, and S. Axelsson, \"PAYSIM: A Financial Mobile Money Simulator for Fraud Detection,\" Blekinge Institute of Technology and The Norwegian University of Science and Technology, 2023. [Online]. Available: https://www.researchgate.net/publication/313138956_PAYSIM_A_FINANCIAL_MOBILE_MONEY_SIMULATOR_FOR_FRAUD_DETECTION [Accessed: October 25, 2023]\n",
    "\n",
    "[8] J. Brownlee \"Why One-Hot Encode Data in Machine Learning?\" [Online]. Available: https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/ [Accessed: December 1, 2023]\n",
    "\n",
    "[9] Han, J., Kamber, M., & Pei, J. 2014. Data Mining: Concepts and Techniques, 3rd Edition. (Section 8.6.5 Improving Classification Accuracy of Class-Imbalanced Data) Morgan Kaufmann.\n",
    "\n",
    "[10] J. Brownlee \"SMOTE for Imbalanced Classification with Python\" Machine Learning Mastery 2021 [Online] Available: https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/ [Accessed: December 1, 2023]\n",
    "\n",
    "[11] V. Singh, “The Good And Bad Of Naive-Bayes Classifier,” [Online]. Medium, Aug. 28, 2021. https://medium.com/@singhvishal0227/the-good-and-bad-of-naive-bayes-classifier-7b0239c65c84#:~:text=The%20time%20complexity%20of%20Naive (Accessed December 07, 2023).\n",
    "\n",
    "[12] scikit learn, “1.4. Support Vector Machines — scikit-learn 0.20.3 documentation,” [Online]. Scikit-learn.org, 2018. https://scikit-learn.org/stable/modules/svm.html  (Accessed December 07, 2023).\n",
    "\n",
    "‌[13] scikit-learn, “1.10. Decision Trees — scikit-learn 0.22 documentation,” [Online]. Scikit-learn.org, 2009. https://scikit-learn.org/stable/modules/tree.html  (Accessed December 07, 2023)."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2093649,
     "sourceId": 3478314,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30579,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
